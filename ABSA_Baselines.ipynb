{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import necessary libraries and load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/nlp_vir/lib/python3.7/site-packages/nltk/tag/stanford.py:149: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordPOSTagger, self).__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "import xml.etree.ElementTree as ET\n",
    "from lxml import etree\n",
    "from scipy.sparse import hstack\n",
    "import numpy as np\n",
    "import warnings\n",
    "import spacy\n",
    "from os.path import join\n",
    "\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "path_train = \"data/semeval_16/ABSA16_Laptops_Train_English_SB2.xml\"\n",
    "path_test = \"data/semeval_16/EN_LAPT_SB2_TESTB.xml\"\n",
    "\n",
    "#For stanford POS Tagger\n",
    "home = \"stanford-postagger-full-2018-10-16\"\n",
    "from nltk.tag.stanford import StanfordPOSTagger as POS_Tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "_path_to_model = home + \"/models/english-bidirectional-distsim.tagger\"\n",
    "_path_to_jar = home + \"/stanford-postagger.jar\"\n",
    "stanford_tag = POS_Tag(model_filename=_path_to_model, path_to_jar=_path_to_jar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xml parser\n",
    "def get_list(path):\n",
    "    tree=ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    text_list = []\n",
    "    opinion_list = []\n",
    "    for review in root.findall('Review'):\n",
    "        text_string=\"\"\n",
    "        opinion_inner_list=[]\n",
    "        for sent in review.findall('./sentences/sentence'):\n",
    "            text_string= text_string+ \" \"+ sent.find('text').text\n",
    "        text_list.append(text_string)\n",
    "        for opinion in review.findall('./Opinions/Opinion'):\n",
    "            opinion_dict = {\n",
    "                opinion.get('category').replace('#','_'): opinion.get('polarity')\n",
    "            }\n",
    "            opinion_inner_list.append(opinion_dict)\n",
    "        opinion_list.append(opinion_inner_list)\n",
    "    return text_list,opinion_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting only 20 most common aspect.\n",
    "def get_most_common_aspect(opinion_list):\n",
    "    import nltk\n",
    "    opinion= []\n",
    "    for inner_list in opinion_list:\n",
    "        for _dict in inner_list:\n",
    "            for key in _dict:\n",
    "                opinion.append(key)\n",
    "    most_common_aspect = [k for k,v in nltk.FreqDist(opinion).most_common(20)]\n",
    "    return most_common_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate data frame\n",
    "def get_data_frame(text_list,opinion_list,most_common_aspect):\n",
    "    data={'Review':text_list}\n",
    "    df = pd.DataFrame(data)\n",
    "    if opinion_list:\n",
    "        for inner_list in opinion_list:\n",
    "            for _dict in inner_list:\n",
    "                for key in _dict:\n",
    "                    if key in most_common_aspect:\n",
    "                        df.loc[opinion_list.index(inner_list),key]=_dict[key]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate data frame for aspect extraction task\n",
    "def get_aspect_data_frame(df,most_common_aspect):\n",
    "    for common_aspect in most_common_aspect:\n",
    "        df[common_aspect]=df[common_aspect].replace(['positive','negative','neutral','conflict'],[1,1,1,1])\n",
    "    df = df.fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positive_data_frame(df,most_common_aspect):\n",
    "    for common_aspect in most_common_aspect:\n",
    "        df[common_aspect]=df[common_aspect].replace(['positive'],[1])\n",
    "        df[common_aspect]=df[common_aspect].replace(['negative','neutral','conflict'],[0,0,0])\n",
    "    df = df.fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_data_frame(df,most_common_aspect):\n",
    "    for common_aspect in most_common_aspect:\n",
    "        df[common_aspect]=df[common_aspect].replace(['negative'],[1])\n",
    "        df[common_aspect]=df[common_aspect].replace(['positive','neutral','conflict'],[0,0,0])\n",
    "    df = df.fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neutral_data_frame(df,most_common_aspect):\n",
    "    for common_aspect in most_common_aspect:\n",
    "        df[common_aspect]=df[common_aspect].replace(['neutral','conflict'],[1,1])\n",
    "        df[common_aspect]=df[common_aspect].replace(['negative','positive'],[0,0])\n",
    "    df = df.fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To tag using stanford pos tagger\n",
    "def posTag(review):\n",
    "    tagged_text_list=[]\n",
    "    for text in review:\n",
    "        tagged_text_list.append(stanford_tag.tag(word_tokenize(text)))\n",
    "    return tagged_text_list\n",
    "#posTag(\"this is random text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the word with tag- noun,adjective,verb,adverb\n",
    "def filterTag(tagged_review):\n",
    "    final_text_list=[]\n",
    "    for text_list in tagged_review:\n",
    "        final_text=[]\n",
    "        for word,tag in text_list:\n",
    "            if tag in ['NN','NNS','NNP','NNPS','RB','RBR','RBS','JJ','JJR','JJS','VB','VBD','VBG','VBN','VBP','VBZ']:\n",
    "                final_text.append(word)\n",
    "        final_text_list.append(' '.join(final_text))\n",
    "    return final_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_aspect(y,most_common_aspect):\n",
    "    position=[]\n",
    "    for innerlist in y:\n",
    "        position.append([i for i, j in enumerate(innerlist) if j == 1])\n",
    "    sorted_common=sorted(most_common_aspect)\n",
    "    dict_aspect=[]\n",
    "    for innerlist in position:\n",
    "        inner_dict={}\n",
    "        for word in sorted_common:\n",
    "            if sorted_common.index(word) in innerlist:\n",
    "                inner_dict[word]= 5\n",
    "            else:\n",
    "                inner_dict[word]=0\n",
    "        dict_aspect.append(inner_dict)\n",
    "    return dict_aspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1 : Opinion prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 PosTag train and test files and serialize them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stage 1:\n",
    "#Making list to train\n",
    "train_text_list,train_opinion_list = get_list(path_train)\n",
    "most_common_aspect = get_most_common_aspect(train_opinion_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This takes time to tag. Already tagged and saved. So, loading file ...\n",
    "#tagged_text_list_train=posTag(train_text_list)\n",
    "#joblib.dump(tagged_text_list_train, 'tagged_text_list_train.pkl')\n",
    "tagged_text_list_train=joblib.load('d/tagged_text_list_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train list after filter\n",
    "final_train_text_list=filterTag(tagged_text_list_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/nlp_vir/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#get data frame\n",
    "df_train = get_data_frame(final_train_text_list,train_opinion_list,most_common_aspect)\n",
    "df_train_aspect = get_aspect_data_frame(df_train,most_common_aspect)\n",
    "df_train_aspect = df_train_aspect.reindex_axis(sorted(df_train_aspect.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similar for test list\n",
    "test_text_list,test_opinion_list = get_list(path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tagged_text_list_test=posTag(test_text_list)\n",
    "#joblib.dump(tagged_text_list_test, 'tagged_text_list_test.pkl')\n",
    "tagged_text_list_test=joblib.load('d/tagged_text_list_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tagged_text_list_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_text_list=filterTag(tagged_text_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/nlp_vir/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "df_test = get_data_frame(final_test_text_list,test_opinion_list,most_common_aspect)\n",
    "df_test_aspect = get_aspect_data_frame(df_test,most_common_aspect)\n",
    "df_test_aspect = df_test_aspect.reindex_axis(sorted(df_test_aspect.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test_aspect.replace('',1, inplace=True)\n",
    "#df_test_aspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Get necessary dataframes and mumpy arrays for text and labels respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort the data frame according to aspect's name and separate data(X) and target(y)\n",
    "#df_train_aspect = df_train_aspect.sample(frac=1).reset_index(drop=True) #For randoming\n",
    "X_train= df_train_aspect.Review\n",
    "y_train = df_train_aspect.drop('Review',1)\n",
    "\n",
    "#df_test_aspect = df_test_aspect.sample(frac=1).reset_index(drop=True) #For randoming\n",
    "X_test = df_test_aspect.Review\n",
    "y_test = df_test_aspect.drop('Review',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change y_train to numpy array\n",
    "import numpy as np\n",
    "y_train = np.asarray(y_train, dtype=np.int64)\n",
    "y_test = np.asarray(y_test, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Generate word embeddings with sklearn's countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate word vecotors using CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "vect = CountVectorizer(max_df=1.0,stop_words='english')  \n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training - I. Naive Bayes', II. SVC, III. Linear SVC, IV. SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_aspect_dtm\n",
    "#y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create various models. These are multi-label models.\n",
    "nb_classif = OneVsRestClassifier(MultinomialNB()).fit(X_train_dtm, y_train)\n",
    "C = 1.0 #SVregularization parameter\n",
    "svc = OneVsRestClassifier(svm.SVC(kernel='linear', C=C)).fit(X_train_dtm, y_train)\n",
    "lin_svc = OneVsRestClassifier(svm.LinearSVC(C=C)).fit(X_train_dtm, y_train)\n",
    "sgd = OneVsRestClassifier(SGDClassifier()).fit(X_train_dtm,y_train)\n",
    "#xg_model = xgb.XGBClassifier().fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the test data using classifiers\n",
    "y_pred_class = nb_classif.predict(X_test_dtm)\n",
    "y_pred_class_svc = svc.predict(X_test_dtm)\n",
    "y_pred_class_lin_svc = lin_svc.predict(X_test_dtm)\n",
    "y_pred_class_sgd = sgd.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Following code to test metrics of all aspect extraction classifiers\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.025\n",
      "0.05\n",
      "0.05\n",
      "0.0125\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test,y_pred_class))\n",
    "print(metrics.accuracy_score(y_test,y_pred_class_svc))\n",
    "print(metrics.accuracy_score(y_test,y_pred_class_lin_svc))\n",
    "print(metrics.accuracy_score(y_test,y_pred_class_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "0.7112299465240641\n",
      "0.7321937321937322\n",
      "0.696875\n"
     ]
    }
   ],
   "source": [
    "print(metrics.precision_score(y_test,y_pred_class,average='micro'))\n",
    "print(metrics.precision_score(y_test,y_pred_class_svc,average='micro'))\n",
    "print(metrics.precision_score(y_test,y_pred_class_lin_svc,average='micro'))\n",
    "print(metrics.precision_score(y_test,y_pred_class_sgd,average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4576271186440678\n",
      "0.6440677966101694\n",
      "0.6222760290556901\n",
      "0.5399515738498789\n"
     ]
    }
   ],
   "source": [
    "print(metrics.recall_score(y_test,y_pred_class,average='micro'))\n",
    "print(metrics.recall_score(y_test,y_pred_class_svc,average='micro'))\n",
    "print(metrics.recall_score(y_test,y_pred_class_lin_svc,average='micro'))\n",
    "print(metrics.recall_score(y_test,y_pred_class_sgd,average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5684210526315788\n",
      "0.6759847522236341\n",
      "0.6727748691099477\n",
      "0.6084583901773534\n"
     ]
    }
   ],
   "source": [
    "print(metrics.f1_score(y_test,y_pred_class,average='micro'))\n",
    "print(metrics.f1_score(y_test,y_pred_class_svc,average='micro'))\n",
    "print(metrics.f1_score(y_test,y_pred_class_lin_svc,average='micro'))\n",
    "print(metrics.f1_score(y_test,y_pred_class_sgd,average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.14      0.24        14\n",
      "           1       0.71      0.50      0.59        24\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.00      0.00      0.00         4\n",
      "           4       0.00      0.00      0.00        21\n",
      "           5       0.00      0.00      0.00         8\n",
      "           6       0.00      0.00      0.00         7\n",
      "           7       0.76      0.64      0.69        39\n",
      "           8       1.00      1.00      1.00        80\n",
      "           9       0.44      0.17      0.24        24\n",
      "          10       0.62      0.70      0.65        46\n",
      "          11       0.00      0.00      0.00         5\n",
      "          12       0.57      0.30      0.39        27\n",
      "          13       0.57      0.45      0.50        29\n",
      "          14       0.77      0.33      0.47        30\n",
      "          15       0.00      0.00      0.00         4\n",
      "          16       0.00      0.00      0.00         9\n",
      "          17       0.00      0.00      0.00        15\n",
      "          18       0.00      0.00      0.00         4\n",
      "          19       0.60      0.27      0.37        11\n",
      "\n",
      "   micro avg       0.75      0.46      0.57       413\n",
      "   macro avg       0.33      0.22      0.26       413\n",
      "weighted avg       0.57      0.46      0.49       413\n",
      " samples avg       0.78      0.50      0.57       413\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.88        14\n",
      "           1       0.68      0.71      0.69        24\n",
      "           2       0.86      0.50      0.63        12\n",
      "           3       0.12      0.25      0.17         4\n",
      "           4       0.56      0.43      0.49        21\n",
      "           5       0.75      0.38      0.50         8\n",
      "           6       0.20      0.14      0.17         7\n",
      "           7       0.74      0.67      0.70        39\n",
      "           8       1.00      0.97      0.99        80\n",
      "           9       0.63      0.50      0.56        24\n",
      "          10       0.68      0.74      0.71        46\n",
      "          11       0.33      0.40      0.36         5\n",
      "          12       0.83      0.74      0.78        27\n",
      "          13       0.56      0.66      0.60        29\n",
      "          14       0.60      0.40      0.48        30\n",
      "          15       0.67      0.50      0.57         4\n",
      "          16       0.18      0.22      0.20         9\n",
      "          17       0.80      0.27      0.40        15\n",
      "          18       1.00      0.25      0.40         4\n",
      "          19       0.60      0.27      0.37        11\n",
      "\n",
      "   micro avg       0.71      0.64      0.68       413\n",
      "   macro avg       0.63      0.50      0.53       413\n",
      "weighted avg       0.72      0.64      0.67       413\n",
      " samples avg       0.73      0.65      0.66       413\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.88        14\n",
      "           1       0.65      0.71      0.68        24\n",
      "           2       1.00      0.42      0.59        12\n",
      "           3       0.17      0.25      0.20         4\n",
      "           4       0.64      0.43      0.51        21\n",
      "           5       1.00      0.25      0.40         8\n",
      "           6       0.33      0.14      0.20         7\n",
      "           7       0.74      0.67      0.70        39\n",
      "           8       1.00      0.96      0.98        80\n",
      "           9       0.61      0.46      0.52        24\n",
      "          10       0.70      0.72      0.71        46\n",
      "          11       0.25      0.20      0.22         5\n",
      "          12       0.83      0.74      0.78        27\n",
      "          13       0.54      0.66      0.59        29\n",
      "          14       0.61      0.37      0.46        30\n",
      "          15       1.00      0.25      0.40         4\n",
      "          16       0.20      0.22      0.21         9\n",
      "          17       1.00      0.20      0.33        15\n",
      "          18       1.00      0.25      0.40         4\n",
      "          19       0.75      0.27      0.40        11\n",
      "\n",
      "   micro avg       0.73      0.62      0.67       413\n",
      "   macro avg       0.69      0.46      0.51       413\n",
      "weighted avg       0.75      0.62      0.66       413\n",
      " samples avg       0.74      0.63      0.65       413\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.64      0.67        14\n",
      "           1       0.67      0.75      0.71        24\n",
      "           2       1.00      0.17      0.29        12\n",
      "           3       0.25      0.25      0.25         4\n",
      "           4       0.40      0.10      0.15        21\n",
      "           5       0.00      0.00      0.00         8\n",
      "           6       0.00      0.00      0.00         7\n",
      "           7       0.78      0.79      0.78        39\n",
      "           8       1.00      0.97      0.99        80\n",
      "           9       0.54      0.29      0.38        24\n",
      "          10       0.64      0.65      0.65        46\n",
      "          11       0.00      0.00      0.00         5\n",
      "          12       0.69      0.41      0.51        27\n",
      "          13       0.51      0.66      0.58        29\n",
      "          14       0.54      0.23      0.33        30\n",
      "          15       0.50      0.25      0.33         4\n",
      "          16       0.12      0.11      0.12         9\n",
      "          17       0.40      0.13      0.20        15\n",
      "          18       1.00      0.25      0.40         4\n",
      "          19       0.75      0.27      0.40        11\n",
      "\n",
      "   micro avg       0.70      0.54      0.61       413\n",
      "   macro avg       0.52      0.35      0.39       413\n",
      "weighted avg       0.66      0.54      0.57       413\n",
      " samples avg       0.74      0.59      0.61       413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    print(metrics.classification_report(y_test, y_pred_class))\n",
    "    print(metrics.classification_report(y_test, y_pred_class_svc))\n",
    "    print(metrics.classification_report(y_test, y_pred_class_lin_svc))\n",
    "    print(metrics.classification_report(y_test, y_pred_class_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 : Aspect category prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stage 2:\n",
    "#Generating extra feature that indicates which aspect category is present in the review\n",
    "train_dict_aspect=get_dict_aspect(y_train, most_common_aspect)\n",
    "d_train=DictVectorizer() \n",
    "X_train_aspect_dtm = d_train.fit_transform(train_dict_aspect)\n",
    "\n",
    "#y_test is used to generated extra feature in order to test the performance of 2nd classifer.\n",
    "#Use y_pred_class_svc(Highest performer for aspect classification) as input for extra feature to test the overall performace.\n",
    "test_dict_aspect=get_dict_aspect(y_test,most_common_aspect)\n",
    "d_test=DictVectorizer() \n",
    "X_test_aspect_dtm = d_test.fit_transform(test_dict_aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for classiflying positive,negative or neutral sentiment of all the aspects\n",
    "def classify_sentiment(df_train,df_test,X_train_aspect_dtm,X_test_aspect_dtm):\n",
    "    \n",
    "    df_train = df_train.reindex_axis(sorted(df_train_positive.columns), axis=1)\n",
    "    df_test = df_test.reindex_axis(sorted(df_test_positive.columns), axis=1)\n",
    "    \n",
    "    df_test.replace('',1, inplace=True)\n",
    "    #print(df_test.iloc[0])\n",
    "\n",
    "    import numpy as np\n",
    "    X_train = df_train.Review\n",
    "    y_train = df_train.drop('Review',1)\n",
    "    y_train = np.asarray(y_train, dtype=np.int64)\n",
    "\n",
    "    X_test = df_test.Review\n",
    "    y_test = df_test.drop('Review',1)\n",
    "    y_test = np.asarray(y_test, dtype=np.int64)\n",
    "\n",
    "    vect_sen = CountVectorizer(stop_words='english',ngram_range=(1,2))  \n",
    "    X_train_dtm = vect_sen.fit_transform(X_train)\n",
    "    X_test_dtm = vect_sen.transform(X_test)\n",
    "\n",
    "    #combining word vector with extra feature.\n",
    "    from scipy.sparse import hstack\n",
    "    X_train_dtm=hstack((X_train_dtm, X_train_aspect_dtm))\n",
    "    X_test_dtm=hstack((X_test_dtm, X_test_aspect_dtm))\n",
    "\n",
    "    C = 1.0 #SVregularization parameter\n",
    "    nb_classif = OneVsRestClassifier(MultinomialNB()).fit(X_train_dtm, y_train)\n",
    "    svc = OneVsRestClassifier(svm.SVC(kernel='linear', C=C)).fit(X_train_dtm, y_train)\n",
    "    lin_svc = OneVsRestClassifier(svm.LinearSVC(C=C)).fit(X_train_dtm, y_train)\n",
    "    sgd = OneVsRestClassifier(SGDClassifier()).fit(X_train_dtm,y_train)\n",
    "\n",
    "    y_pred_class= nb_classif.predict(X_test_dtm)\n",
    "    y_pred_class_svc = svc.predict(X_test_dtm)\n",
    "    y_pred_class_lin_svc = lin_svc.predict(X_test_dtm)\n",
    "    y_pred_class_sgd = sgd.predict(X_test_dtm)\n",
    "    return (y_test,y_pred_class,y_pred_class_svc,y_pred_class_lin_svc,y_pred_class_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrices(y_test,y_pred_class,y_pred_class_svc,y_pred_class_lin_svc,y_pred_class_sgd):\n",
    "    print(\"Accuracy:\")\n",
    "    print(metrics.accuracy_score(y_test,y_pred_class))\n",
    "    print(metrics.accuracy_score(y_test,y_pred_class_svc))\n",
    "    print(metrics.accuracy_score(y_test,y_pred_class_lin_svc))\n",
    "    print(metrics.accuracy_score(y_test,y_pred_class_sgd))\n",
    "\n",
    "    print(\"\\nAverage precision:\")\n",
    "    print(metrics.precision_score(y_test,y_pred_class,average='micro'))\n",
    "    print(metrics.precision_score(y_test,y_pred_class_svc,average='micro'))\n",
    "    print(metrics.precision_score(y_test,y_pred_class_lin_svc,average='micro'))\n",
    "    print(metrics.precision_score(y_test,y_pred_class_sgd,average='micro'))\n",
    "\n",
    "    print(\"\\nAverage recall:\")\n",
    "    print(metrics.recall_score(y_test,y_pred_class,average='micro'))\n",
    "    print(metrics.recall_score(y_test,y_pred_class_svc,average='micro'))\n",
    "    print(metrics.recall_score(y_test,y_pred_class_lin_svc,average='micro'))\n",
    "    print(metrics.recall_score(y_test,y_pred_class_sgd,average='micro'))\n",
    "    \n",
    "    print(\"\\nAverage f1:\")\n",
    "    print(metrics.f1_score(y_test,y_pred_class,average='micro'))\n",
    "    print(metrics.f1_score(y_test,y_pred_class_svc,average='micro'))\n",
    "    print(metrics.f1_score(y_test,y_pred_class_lin_svc,average='micro'))\n",
    "    print(metrics.f1_score(y_test,y_pred_class_sgd,average='micro'))\n",
    "\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(metrics.classification_report(y_test, y_pred_class))\n",
    "    print(metrics.classification_report(y_test, y_pred_class_svc))\n",
    "    print(metrics.classification_report(y_test, y_pred_class_lin_svc))\n",
    "    print(metrics.classification_report(y_test, y_pred_class_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/nlp_vir/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
      "  after removing the cwd from sys.path.\n",
      "/anaconda3/envs/nlp_vir/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.025\n",
      "0.325\n",
      "0.3\n",
      "0.225\n",
      "\n",
      "Average precision:\n",
      "1.0\n",
      "0.976271186440678\n",
      "0.9959349593495935\n",
      "1.0\n",
      "\n",
      "Average recall:\n",
      "0.2033898305084746\n",
      "0.6973365617433414\n",
      "0.5932203389830508\n",
      "0.5835351089588378\n",
      "\n",
      "Average f1:\n",
      "0.33802816901408456\n",
      "0.8135593220338984\n",
      "0.7435508345978756\n",
      "0.7370030581039755\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        14\n",
      "           1       0.00      0.00      0.00        24\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.00      0.00      0.00         4\n",
      "           4       0.00      0.00      0.00        21\n",
      "           5       0.00      0.00      0.00         8\n",
      "           6       0.00      0.00      0.00         7\n",
      "           7       0.00      0.00      0.00        39\n",
      "           8       1.00      0.88      0.93        80\n",
      "           9       0.00      0.00      0.00        24\n",
      "          10       1.00      0.30      0.47        46\n",
      "          11       0.00      0.00      0.00         5\n",
      "          12       0.00      0.00      0.00        27\n",
      "          13       0.00      0.00      0.00        29\n",
      "          14       0.00      0.00      0.00        30\n",
      "          15       0.00      0.00      0.00         4\n",
      "          16       0.00      0.00      0.00         9\n",
      "          17       0.00      0.00      0.00        15\n",
      "          18       0.00      0.00      0.00         4\n",
      "          19       0.00      0.00      0.00        11\n",
      "\n",
      "   micro avg       1.00      0.20      0.34       413\n",
      "   macro avg       0.10      0.06      0.07       413\n",
      "weighted avg       0.31      0.20      0.23       413\n",
      " samples avg       0.88      0.24      0.36       413\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.79      0.88        14\n",
      "           1       0.93      0.58      0.72        24\n",
      "           2       1.00      1.00      1.00        12\n",
      "           3       1.00      1.00      1.00         4\n",
      "           4       0.94      0.76      0.84        21\n",
      "           5       1.00      0.50      0.67         8\n",
      "           6       0.67      0.29      0.40         7\n",
      "           7       1.00      0.72      0.84        39\n",
      "           8       1.00      0.80      0.89        80\n",
      "           9       1.00      0.79      0.88        24\n",
      "          10       0.97      0.76      0.85        46\n",
      "          11       1.00      0.80      0.89         5\n",
      "          12       1.00      0.56      0.71        27\n",
      "          13       0.94      0.59      0.72        29\n",
      "          14       1.00      0.93      0.97        30\n",
      "          15       0.67      0.50      0.57         4\n",
      "          16       0.75      0.33      0.46         9\n",
      "          17       1.00      0.33      0.50        15\n",
      "          18       1.00      0.75      0.86         4\n",
      "          19       1.00      0.18      0.31        11\n",
      "\n",
      "   micro avg       0.98      0.70      0.81       413\n",
      "   macro avg       0.94      0.65      0.75       413\n",
      "weighted avg       0.97      0.70      0.80       413\n",
      " samples avg       0.87      0.68      0.74       413\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.64      0.78        14\n",
      "           1       0.93      0.54      0.68        24\n",
      "           2       1.00      0.42      0.59        12\n",
      "           3       1.00      1.00      1.00         4\n",
      "           4       1.00      0.62      0.76        21\n",
      "           5       1.00      0.12      0.22         8\n",
      "           6       0.00      0.00      0.00         7\n",
      "           7       1.00      0.64      0.78        39\n",
      "           8       1.00      0.80      0.89        80\n",
      "           9       1.00      0.75      0.86        24\n",
      "          10       1.00      0.76      0.86        46\n",
      "          11       1.00      0.40      0.57         5\n",
      "          12       1.00      0.41      0.58        27\n",
      "          13       1.00      0.52      0.68        29\n",
      "          14       1.00      0.70      0.82        30\n",
      "          15       1.00      0.25      0.40         4\n",
      "          16       0.00      0.00      0.00         9\n",
      "          17       1.00      0.27      0.42        15\n",
      "          18       1.00      0.75      0.86         4\n",
      "          19       1.00      0.09      0.17        11\n",
      "\n",
      "   micro avg       1.00      0.59      0.74       413\n",
      "   macro avg       0.90      0.48      0.60       413\n",
      "weighted avg       0.96      0.59      0.71       413\n",
      " samples avg       0.87      0.60      0.68       413\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.64      0.78        14\n",
      "           1       1.00      0.50      0.67        24\n",
      "           2       1.00      0.42      0.59        12\n",
      "           3       1.00      1.00      1.00         4\n",
      "           4       1.00      0.76      0.86        21\n",
      "           5       1.00      0.38      0.55         8\n",
      "           6       1.00      0.43      0.60         7\n",
      "           7       1.00      0.64      0.78        39\n",
      "           8       1.00      0.78      0.87        80\n",
      "           9       1.00      0.62      0.77        24\n",
      "          10       1.00      0.61      0.76        46\n",
      "          11       1.00      0.60      0.75         5\n",
      "          12       1.00      0.44      0.62        27\n",
      "          13       1.00      0.38      0.55        29\n",
      "          14       1.00      0.83      0.91        30\n",
      "          15       1.00      0.25      0.40         4\n",
      "          16       1.00      0.11      0.20         9\n",
      "          17       1.00      0.07      0.12        15\n",
      "          18       1.00      0.75      0.86         4\n",
      "          19       1.00      0.18      0.31        11\n",
      "\n",
      "   micro avg       1.00      0.58      0.74       413\n",
      "   macro avg       1.00      0.52      0.65       413\n",
      "weighted avg       1.00      0.58      0.71       413\n",
      " samples avg       0.86      0.57      0.66       413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#For positive sentiment classifier\n",
    "df_train = get_data_frame(final_train_text_list,train_opinion_list,most_common_aspect)\n",
    "df_test = get_data_frame(final_test_text_list,test_opinion_list,most_common_aspect)\n",
    "\n",
    "df_train_positive = get_positive_data_frame(df_train,most_common_aspect)\n",
    "df_test_positive = get_positive_data_frame(df_test,most_common_aspect)\n",
    "y_test_pos,y_pred_class_pos,y_pred_class_svc_pos,y_pred_class_lin_svc_pos,y_pred_class_sgd_pos=classify_sentiment(df_train_positive,df_test_positive,X_train_aspect_dtm,X_test_aspect_dtm)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    print_metrices(y_test_pos,y_pred_class_pos,y_pred_class_svc_pos,y_pred_class_lin_svc_pos,y_pred_class_sgd_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/nlp_vir/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
      "  after removing the cwd from sys.path.\n",
      "/anaconda3/envs/nlp_vir/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.0\n",
      "0.075\n",
      "0.05\n",
      "0.0875\n",
      "\n",
      "Average precision:\n",
      "1.0\n",
      "0.9861111111111112\n",
      "1.0\n",
      "0.9824561403508771\n",
      "\n",
      "Average recall:\n",
      "0.024213075060532687\n",
      "0.17191283292978207\n",
      "0.13075060532687652\n",
      "0.13559322033898305\n",
      "\n",
      "Average f1:\n",
      "0.04728132387706856\n",
      "0.29278350515463913\n",
      "0.23126338329764454\n",
      "0.23829787234042552\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        14\n",
      "           1       0.00      0.00      0.00        24\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.00      0.00      0.00         4\n",
      "           4       0.00      0.00      0.00        21\n",
      "           5       0.00      0.00      0.00         8\n",
      "           6       0.00      0.00      0.00         7\n",
      "           7       0.00      0.00      0.00        39\n",
      "           8       1.00      0.11      0.20        80\n",
      "           9       0.00      0.00      0.00        24\n",
      "          10       0.00      0.00      0.00        46\n",
      "          11       0.00      0.00      0.00         5\n",
      "          12       0.00      0.00      0.00        27\n",
      "          13       1.00      0.03      0.07        29\n",
      "          14       0.00      0.00      0.00        30\n",
      "          15       0.00      0.00      0.00         4\n",
      "          16       0.00      0.00      0.00         9\n",
      "          17       0.00      0.00      0.00        15\n",
      "          18       0.00      0.00      0.00         4\n",
      "          19       0.00      0.00      0.00        11\n",
      "\n",
      "   micro avg       1.00      0.02      0.05       413\n",
      "   macro avg       0.10      0.01      0.01       413\n",
      "weighted avg       0.26      0.02      0.04       413\n",
      " samples avg       0.11      0.03      0.05       413\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.14      0.25        14\n",
      "           1       1.00      0.38      0.55        24\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.00      0.00      0.00         4\n",
      "           4       1.00      0.10      0.17        21\n",
      "           5       1.00      0.38      0.55         8\n",
      "           6       1.00      0.29      0.44         7\n",
      "           7       1.00      0.03      0.05        39\n",
      "           8       1.00      0.16      0.28        80\n",
      "           9       1.00      0.08      0.15        24\n",
      "          10       1.00      0.20      0.33        46\n",
      "          11       1.00      0.20      0.33         5\n",
      "          12       0.75      0.11      0.19        27\n",
      "          13       1.00      0.34      0.51        29\n",
      "          14       0.00      0.00      0.00        30\n",
      "          15       0.00      0.00      0.00         4\n",
      "          16       1.00      0.67      0.80         9\n",
      "          17       0.00      0.00      0.00        15\n",
      "          18       0.00      0.00      0.00         4\n",
      "          19       1.00      0.73      0.84        11\n",
      "\n",
      "   micro avg       0.99      0.17      0.29       413\n",
      "   macro avg       0.69      0.19      0.27       413\n",
      "weighted avg       0.82      0.17      0.26       413\n",
      " samples avg       0.41      0.20      0.24       413\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.07      0.13        14\n",
      "           1       1.00      0.25      0.40        24\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.00      0.00      0.00         4\n",
      "           4       1.00      0.14      0.25        21\n",
      "           5       1.00      0.25      0.40         8\n",
      "           6       1.00      0.14      0.25         7\n",
      "           7       0.00      0.00      0.00        39\n",
      "           8       1.00      0.16      0.28        80\n",
      "           9       0.00      0.00      0.00        24\n",
      "          10       1.00      0.15      0.26        46\n",
      "          11       0.00      0.00      0.00         5\n",
      "          12       1.00      0.04      0.07        27\n",
      "          13       1.00      0.34      0.51        29\n",
      "          14       0.00      0.00      0.00        30\n",
      "          15       0.00      0.00      0.00         4\n",
      "          16       1.00      0.44      0.62         9\n",
      "          17       0.00      0.00      0.00        15\n",
      "          18       0.00      0.00      0.00         4\n",
      "          19       1.00      0.55      0.71        11\n",
      "\n",
      "   micro avg       1.00      0.13      0.23       413\n",
      "   macro avg       0.55      0.13      0.19       413\n",
      "weighted avg       0.67      0.13      0.21       413\n",
      " samples avg       0.33      0.16      0.20       413\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.07      0.13        14\n",
      "           1       1.00      0.29      0.45        24\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.00      0.00      0.00         4\n",
      "           4       1.00      0.10      0.17        21\n",
      "           5       0.00      0.00      0.00         8\n",
      "           6       1.00      0.43      0.60         7\n",
      "           7       1.00      0.03      0.05        39\n",
      "           8       1.00      0.20      0.33        80\n",
      "           9       0.00      0.00      0.00        24\n",
      "          10       1.00      0.20      0.33        46\n",
      "          11       0.00      0.00      0.00         5\n",
      "          12       0.00      0.00      0.00        27\n",
      "          13       1.00      0.24      0.39        29\n",
      "          14       0.00      0.00      0.00        30\n",
      "          15       0.00      0.00      0.00         4\n",
      "          16       1.00      0.33      0.50         9\n",
      "          17       0.00      0.00      0.00        15\n",
      "          18       0.00      0.00      0.00         4\n",
      "          19       0.88      0.64      0.74        11\n",
      "\n",
      "   micro avg       0.98      0.14      0.24       413\n",
      "   macro avg       0.49      0.13      0.18       413\n",
      "weighted avg       0.67      0.14      0.21       413\n",
      " samples avg       0.32      0.19      0.22       413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#For negative sentiment classifier\n",
    "df_train = get_data_frame(final_train_text_list,train_opinion_list,most_common_aspect)\n",
    "df_test = get_data_frame(final_test_text_list,test_opinion_list,most_common_aspect)\n",
    "\n",
    "df_train_neg = get_negative_data_frame(df_train,most_common_aspect)\n",
    "df_test_neg = get_negative_data_frame(df_test,most_common_aspect)\n",
    "\n",
    "y_test_neg,y_pred_class_neg,y_pred_class_svc_neg,y_pred_class_lin_svc_neg,y_pred_class_sgd_neg=classify_sentiment(df_train_neg,df_test_neg,X_train_aspect_dtm,X_test_aspect_dtm)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    print_metrices(y_test_neg,y_pred_class_neg,y_pred_class_svc_neg,y_pred_class_lin_svc_neg,y_pred_class_sgd_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/nlp_vir/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
      "  after removing the cwd from sys.path.\n",
      "/anaconda3/envs/nlp_vir/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "Average precision:\n",
      "0.0\n",
      "0.9230769230769231\n",
      "1.0\n",
      "1.0\n",
      "\n",
      "Average recall:\n",
      "0.0\n",
      "0.029055690072639227\n",
      "0.007263922518159807\n",
      "0.007263922518159807\n",
      "\n",
      "Average f1:\n",
      "0.0\n",
      "0.05633802816901409\n",
      "0.014423076923076922\n",
      "0.014423076923076922\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        14\n",
      "           1       0.00      0.00      0.00        24\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.00      0.00      0.00         4\n",
      "           4       0.00      0.00      0.00        21\n",
      "           5       0.00      0.00      0.00         8\n",
      "           6       0.00      0.00      0.00         7\n",
      "           7       0.00      0.00      0.00        39\n",
      "           8       0.00      0.00      0.00        80\n",
      "           9       0.00      0.00      0.00        24\n",
      "          10       0.00      0.00      0.00        46\n",
      "          11       0.00      0.00      0.00         5\n",
      "          12       0.00      0.00      0.00        27\n",
      "          13       0.00      0.00      0.00        29\n",
      "          14       0.00      0.00      0.00        30\n",
      "          15       0.00      0.00      0.00         4\n",
      "          16       0.00      0.00      0.00         9\n",
      "          17       0.00      0.00      0.00        15\n",
      "          18       0.00      0.00      0.00         4\n",
      "          19       0.00      0.00      0.00        11\n",
      "\n",
      "   micro avg       0.00      0.00      0.00       413\n",
      "   macro avg       0.00      0.00      0.00       413\n",
      "weighted avg       0.00      0.00      0.00       413\n",
      " samples avg       0.00      0.00      0.00       413\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        14\n",
      "           1       0.00      0.00      0.00        24\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.00      0.00      0.00         4\n",
      "           4       0.00      0.00      0.00        21\n",
      "           5       0.00      0.00      0.00         8\n",
      "           6       0.00      0.00      0.00         7\n",
      "           7       1.00      0.18      0.30        39\n",
      "           8       0.00      0.00      0.00        80\n",
      "           9       0.00      0.00      0.00        24\n",
      "          10       1.00      0.04      0.08        46\n",
      "          11       0.00      0.00      0.00         5\n",
      "          12       1.00      0.04      0.07        27\n",
      "          13       0.00      0.00      0.00        29\n",
      "          14       0.00      0.00      0.00        30\n",
      "          15       0.00      0.00      0.00         4\n",
      "          16       0.00      0.00      0.00         9\n",
      "          17       1.00      0.13      0.24        15\n",
      "          18       0.00      0.00      0.00         4\n",
      "          19       0.00      0.00      0.00        11\n",
      "\n",
      "   micro avg       0.92      0.03      0.06       413\n",
      "   macro avg       0.20      0.02      0.03       413\n",
      "weighted avg       0.31      0.03      0.05       413\n",
      " samples avg       0.11      0.02      0.03       413\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        14\n",
      "           1       0.00      0.00      0.00        24\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.00      0.00      0.00         4\n",
      "           4       0.00      0.00      0.00        21\n",
      "           5       0.00      0.00      0.00         8\n",
      "           6       0.00      0.00      0.00         7\n",
      "           7       1.00      0.08      0.14        39\n",
      "           8       0.00      0.00      0.00        80\n",
      "           9       0.00      0.00      0.00        24\n",
      "          10       0.00      0.00      0.00        46\n",
      "          11       0.00      0.00      0.00         5\n",
      "          12       0.00      0.00      0.00        27\n",
      "          13       0.00      0.00      0.00        29\n",
      "          14       0.00      0.00      0.00        30\n",
      "          15       0.00      0.00      0.00         4\n",
      "          16       0.00      0.00      0.00         9\n",
      "          17       0.00      0.00      0.00        15\n",
      "          18       0.00      0.00      0.00         4\n",
      "          19       0.00      0.00      0.00        11\n",
      "\n",
      "   micro avg       1.00      0.01      0.01       413\n",
      "   macro avg       0.05      0.00      0.01       413\n",
      "weighted avg       0.09      0.01      0.01       413\n",
      " samples avg       0.04      0.01      0.01       413\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        14\n",
      "           1       0.00      0.00      0.00        24\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.00      0.00      0.00         4\n",
      "           4       0.00      0.00      0.00        21\n",
      "           5       0.00      0.00      0.00         8\n",
      "           6       0.00      0.00      0.00         7\n",
      "           7       1.00      0.05      0.10        39\n",
      "           8       0.00      0.00      0.00        80\n",
      "           9       0.00      0.00      0.00        24\n",
      "          10       0.00      0.00      0.00        46\n",
      "          11       0.00      0.00      0.00         5\n",
      "          12       0.00      0.00      0.00        27\n",
      "          13       0.00      0.00      0.00        29\n",
      "          14       1.00      0.03      0.06        30\n",
      "          15       0.00      0.00      0.00         4\n",
      "          16       0.00      0.00      0.00         9\n",
      "          17       0.00      0.00      0.00        15\n",
      "          18       0.00      0.00      0.00         4\n",
      "          19       0.00      0.00      0.00        11\n",
      "\n",
      "   micro avg       1.00      0.01      0.01       413\n",
      "   macro avg       0.10      0.00      0.01       413\n",
      "weighted avg       0.17      0.01      0.01       413\n",
      " samples avg       0.04      0.01      0.01       413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#For neutral or conflict sentiment classifier\n",
    "df_train = get_data_frame(final_train_text_list,train_opinion_list,most_common_aspect)\n",
    "df_test = get_data_frame(final_test_text_list,test_opinion_list,most_common_aspect)\n",
    "\n",
    "df_train_neu = get_neutral_data_frame(df_train,most_common_aspect)\n",
    "df_test_neu = get_neutral_data_frame(df_test,most_common_aspect)\n",
    "\n",
    "y_test_neu,y_pred_class_neu,y_pred_class_svc_neu,y_pred_class_lin_svc_neu,y_pred_class_sgd_neu=classify_sentiment(df_train_neu,df_test_neu,X_train_aspect_dtm,X_test_aspect_dtm)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    print_metrices(y_test_neu,y_pred_class_neu,y_pred_class_svc_neu,y_pred_class_lin_svc_neu,y_pred_class_sgd_neu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full pipeline : ABSA of user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a laptop review:\n",
      "\n",
      "This is my first asus laptop. So far i am really enjoying this laptop. 512GB SSD is super fast. Battery life is also good and can last very long. I have no complain on screen quality too as display supports 4k videos. Maybe that is why it costs a lot. This is an expensive laptop and it's price is very high compared to other laptops of similar specs. So, if you have no trouble paying for this laptop, it is pretty good.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Aspect Based Sentiment analyis of user's input.\n",
    "user_input=input(\"Enter a laptop review:\\n\\n\")\n",
    "#Enter a laptop review:\n",
    "\n",
    "#This is my first asus laptop. So far i am really enjoying this laptop. 512GB SSD is super fast. \n",
    "#Battery life is also good and can last very long. \n",
    "#I have no complain on screen quality too as display supports 4k videos. Maybe that is why it costs a lot. \n",
    "#This is an expensive laptop and it's price is very high compared to other laptops of similar specs. \n",
    "#So, if you have no trouble paying for this laptop, it is pretty good.\n",
    "\n",
    "#Preprocessing and vectorizing\n",
    "tagged_user_input = posTag([user_input])\n",
    "filter_tagged_user_input = filterTag(tagged_user_input)\n",
    "\n",
    "user_input_series=pd.Series(filter_tagged_user_input)\n",
    "user_input_series_dtm=vect.transform(user_input_series)\n",
    "\n",
    "predict_aspect= svc.predict(user_input_series_dtm)\n",
    "extra_feature=get_dict_aspect(predict_aspect, most_common_aspect)\n",
    "extra_feature_dtm=DictVectorizer().fit_transform(extra_feature)\n",
    "predict_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/nlp_vir/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
      "  after removing the cwd from sys.path.\n",
      "/anaconda3/envs/nlp_vir/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predicting weather the dectected aspect is positive or not\n",
    "test_opinion_list=[]\n",
    "df_test = get_data_frame(filter_tagged_user_input,test_opinion_list,most_common_aspect)\n",
    "df_train = get_data_frame(final_train_text_list,train_opinion_list,most_common_aspect)\n",
    "\n",
    "df_train_positive = get_positive_data_frame(df_train,most_common_aspect)\n",
    "y_test_pos,y_pred_class_pos,y_pred_class_svc_pos,y_pred_class_lin_svc_pos,y_pred_class_sgd_pos=classify_sentiment(df_train_positive,df_test,X_train_aspect_dtm,extra_feature_dtm)\n",
    "\n",
    "y_pred_class_svc_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/nlp_vir/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
      "  after removing the cwd from sys.path.\n",
      "/anaconda3/envs/nlp_vir/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predicting weather the dectected aspect is negative or not\n",
    "test_opinion_list=[]\n",
    "df_test = get_data_frame(filter_tagged_user_input,test_opinion_list,most_common_aspect)\n",
    "df_train = get_data_frame(final_train_text_list,train_opinion_list,most_common_aspect)\n",
    "\n",
    "df_train_negative = get_negative_data_frame(df_train,most_common_aspect)\n",
    "y_test_neg,y_pred_class_neg,y_pred_class_svc_neg,y_pred_class_lin_svc_neg,y_pred_class_sgd_neg=classify_sentiment(df_train_negative,df_test,X_train_aspect_dtm,extra_feature_dtm)\n",
    "\n",
    "y_pred_class_svc_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/nlp_vir/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
      "  after removing the cwd from sys.path.\n",
      "/anaconda3/envs/nlp_vir/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predicting weather the dectected aspect is neutral or coflict or not\n",
    "test_opinion_list=[]\n",
    "df_test = get_data_frame(filter_tagged_user_input,test_opinion_list,most_common_aspect)\n",
    "df_train = get_data_frame(final_train_text_list,train_opinion_list,most_common_aspect)\n",
    "\n",
    "df_train_neutral = get_neutral_data_frame(df_train,most_common_aspect)\n",
    "y_test_neu,y_pred_class_neu,y_pred_class_svc_neu,y_pred_class_lin_svc_neu,y_pred_class_sgd_neu=classify_sentiment(df_train_neutral,df_test,X_train_aspect_dtm,extra_feature_dtm)\n",
    "\n",
    "y_pred_class_svc_neu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 8, 10, 13]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding the aspect that is positive\n",
    "index_positive=[]\n",
    "for i, (a, b) in enumerate(zip(predict_aspect.tolist()[0], y_pred_class_svc_pos.tolist()[0])):\n",
    "    if a ==1 and b==1:\n",
    "        index_positive.append(i)\n",
    "index_positive         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding the aspect that is negative\n",
    "index_negative=[]\n",
    "for i, (a, b) in enumerate(zip(predict_aspect.tolist()[0], y_pred_class_svc_neg.tolist()[0])):\n",
    "    if a ==1 and b==1:\n",
    "        index_negative.append(i)\n",
    "index_negative         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding the aspect that is neutral\n",
    "index_neutral=[]\n",
    "for i, (a, b) in enumerate(zip(predict_aspect.tolist()[0], y_pred_class_svc_neu.tolist()[0])):\n",
    "    if a ==1 and b==1:\n",
    "        index_neutral.append(i)\n",
    "index_neutral         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "if index_positive:\n",
    "    for index in index_positive:\n",
    "        output.append(sorted(most_common_aspect)[index]+\": positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "if index_negative:\n",
    "    for index in index_negative:\n",
    "        output.append(sorted(most_common_aspect)[index]+\": negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "if index_neutral:\n",
    "    for index in index_neutral:\n",
    "        output.append(sorted(most_common_aspect)[index]+\": neutral or conflict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BATTERY_OPERATION_PERFORMANCE: positive',\n",
       " 'DISPLAY_GENERAL: positive',\n",
       " 'LAPTOP_GENERAL: positive',\n",
       " 'LAPTOP_OPERATION_PERFORMANCE: positive',\n",
       " 'LAPTOP_QUALITY: positive',\n",
       " 'LAPTOP_PRICE: neutral or conflict']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prediction of Aspect Based Sentiment Analaysis for user's input\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = joblib.load('processed_data/organic_reduced/code_to_embed.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3954, 300)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex2 = joblib.load('processed_data/organic_reduced/code_to_vocab.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3953: '<UNK>',\n",
       " 0: '<PAD>',\n",
       " 1: '.',\n",
       " 2: 'the',\n",
       " 3: ',',\n",
       " 4: 'and',\n",
       " 5: 'I',\n",
       " 6: 'a',\n",
       " 7: 'is',\n",
       " 8: 'to',\n",
       " 9: 'was',\n",
       " 10: 'of',\n",
       " 11: 'The',\n",
       " 12: 'it',\n",
       " 13: 'for',\n",
       " 14: '!',\n",
       " 15: 'in',\n",
       " 16: 'food',\n",
       " 17: 'you',\n",
       " 18: 'but',\n",
       " 19: 'place',\n",
       " 20: '-',\n",
       " 21: 'that',\n",
       " 22: 'not',\n",
       " 23: 'this',\n",
       " 24: 'good',\n",
       " 25: 'on',\n",
       " 26: 'we',\n",
       " 27: 'with',\n",
       " 28: \"n't\",\n",
       " 29: 'great',\n",
       " 30: 'are',\n",
       " 31: 'have',\n",
       " 32: 'were',\n",
       " 33: 'my',\n",
       " 34: 'service',\n",
       " 35: 'had',\n",
       " 36: \"'s\",\n",
       " 37: 'at',\n",
       " 38: 'so',\n",
       " 39: 'very',\n",
       " 40: 'restaurant',\n",
       " 41: 'be',\n",
       " 42: 'We',\n",
       " 43: ')',\n",
       " 44: 'they',\n",
       " 45: 'all',\n",
       " 46: 'there',\n",
       " 47: 'go',\n",
       " 48: '(',\n",
       " 49: '...',\n",
       " 50: 'as',\n",
       " 51: 'like',\n",
       " 52: 'would',\n",
       " 53: 'do',\n",
       " 54: 'here',\n",
       " 55: 'or',\n",
       " 56: 'This',\n",
       " 57: 'which',\n",
       " 58: 'back',\n",
       " 59: 'by',\n",
       " 60: 'out',\n",
       " 61: 'one',\n",
       " 62: 'our',\n",
       " 63: 'if',\n",
       " 64: 'can',\n",
       " 65: 'time',\n",
       " 66: 'your',\n",
       " 67: 'just',\n",
       " 68: 'from',\n",
       " 69: 'more',\n",
       " 70: 'been',\n",
       " 71: 'an',\n",
       " 72: 'about',\n",
       " 73: 'get',\n",
       " 74: 'delicious',\n",
       " 75: 'best',\n",
       " 76: 'me',\n",
       " 77: 'It',\n",
       " 78: 'i',\n",
       " 79: 'us',\n",
       " 80: 'too',\n",
       " 81: 'their',\n",
       " 82: 'staff',\n",
       " 83: 'never',\n",
       " 84: \"'ve\",\n",
       " 85: 'no',\n",
       " 86: '$',\n",
       " 87: 'than',\n",
       " 88: 'will',\n",
       " 89: 'pizza',\n",
       " 90: 'really',\n",
       " 91: 'has',\n",
       " 92: 'did',\n",
       " 93: 'nice',\n",
       " 94: 'My',\n",
       " 95: 'always',\n",
       " 96: 'menu',\n",
       " 97: 'what',\n",
       " 98: 'dinner',\n",
       " 99: 'well',\n",
       " 100: 'only',\n",
       " 101: 'up',\n",
       " 102: 'excellent',\n",
       " 103: 'when',\n",
       " 104: 'wine',\n",
       " 105: 'try',\n",
       " 106: 'recommend',\n",
       " 107: 'eat',\n",
       " 108: 'even',\n",
       " 109: 'some',\n",
       " 110: '\"',\n",
       " 111: 'sushi',\n",
       " 112: 'table',\n",
       " 113: 'make',\n",
       " 114: 'wait',\n",
       " 115: 'worth',\n",
       " 116: 'restaurants',\n",
       " 117: 'other',\n",
       " 118: 'times',\n",
       " 119: 'Great',\n",
       " 120: 'night',\n",
       " 121: 'little',\n",
       " 122: 'experience',\n",
       " 123: 'atmosphere',\n",
       " 124: 'who',\n",
       " 125: 'because',\n",
       " 126: 'got',\n",
       " 127: 'better',\n",
       " 128: 'A',\n",
       " 129: 'order',\n",
       " 130: 'went',\n",
       " 131: 'much',\n",
       " 132: 'prices',\n",
       " 133: 'dishes',\n",
       " 134: 'price',\n",
       " 135: 'again',\n",
       " 136: 'special',\n",
       " 137: 'friendly',\n",
       " 138: 'going',\n",
       " 139: 'fresh',\n",
       " 140: 'over',\n",
       " 141: ':',\n",
       " 142: 'made',\n",
       " 143: 'They',\n",
       " 144: 'many',\n",
       " 145: 'amazing',\n",
       " 146: 'ever',\n",
       " 147: '?',\n",
       " 148: 'ca',\n",
       " 149: \"'re\",\n",
       " 150: 'way',\n",
       " 151: 'people',\n",
       " 152: 'last',\n",
       " 153: 'say',\n",
       " 154: 'could',\n",
       " 155: 'If',\n",
       " 156: 'most',\n",
       " 157: 'drinks',\n",
       " 158: 'Service',\n",
       " 159: 'nothing',\n",
       " 160: 'around',\n",
       " 161: 'friends',\n",
       " 162: 'friend',\n",
       " 163: 'ordered',\n",
       " 164: 'came',\n",
       " 165: 'am',\n",
       " 166: 'two',\n",
       " 167: 'bad',\n",
       " 168: 'fish',\n",
       " 169: 'also',\n",
       " 170: 'chicken',\n",
       " 171: 'though',\n",
       " 172: 'any',\n",
       " 173: 'before',\n",
       " 174: 'New',\n",
       " 175: 'bit',\n",
       " 176: 'tasty',\n",
       " 177: 'waiter',\n",
       " 178: 'small',\n",
       " 179: 'You',\n",
       " 180: 'decor',\n",
       " 181: 'Thai',\n",
       " 182: 'down',\n",
       " 183: 'favorite',\n",
       " 184: 'first',\n",
       " 185: 'different',\n",
       " 186: 'Indian',\n",
       " 187: 'asked',\n",
       " 188: 'rice',\n",
       " 189: 'visit',\n",
       " 190: 'thing',\n",
       " 191: 'love',\n",
       " 192: 'spot',\n",
       " 193: 'off',\n",
       " 194: \"'m\",\n",
       " 195: 'enjoy',\n",
       " 196: 'city',\n",
       " 197: 'NYC',\n",
       " 198: 'sure',\n",
       " 199: 'lot',\n",
       " 200: 'highly',\n",
       " 201: 'he',\n",
       " 202: 'want',\n",
       " 203: 'does',\n",
       " 204: 'meal',\n",
       " 205: 'she',\n",
       " 206: 'average',\n",
       " 207: 'every',\n",
       " 208: 'attentive',\n",
       " 209: 'feel',\n",
       " 210: 'think',\n",
       " 211: 'pretty',\n",
       " 212: 'being',\n",
       " 213: 'hot',\n",
       " 214: 'list',\n",
       " 215: 'away',\n",
       " 216: 'York',\n",
       " 217: 'perfect',\n",
       " 218: 'top',\n",
       " 219: '--',\n",
       " 220: 'romantic',\n",
       " 221: 'both',\n",
       " 222: 'places',\n",
       " 223: 'years',\n",
       " 224: 'waitress',\n",
       " 225: 'Not',\n",
       " 226: 'wonderful',\n",
       " 227: 'minutes',\n",
       " 228: 'Food',\n",
       " 229: 'fun',\n",
       " 230: 'come',\n",
       " 231: 'its',\n",
       " 232: 'then',\n",
       " 233: 'far',\n",
       " 234: 'dessert',\n",
       " 235: 'take',\n",
       " 236: 'But',\n",
       " 237: 'said',\n",
       " 238: 'tried',\n",
       " 239: 'them',\n",
       " 240: 'eaten',\n",
       " 241: ';',\n",
       " 242: 'find',\n",
       " 243: 'bar',\n",
       " 244: 'roll',\n",
       " 245: '..',\n",
       " 246: 'how',\n",
       " 247: 'should',\n",
       " 248: 'must',\n",
       " 249: 'else',\n",
       " 250: '2',\n",
       " 251: 'such',\n",
       " 252: 'once',\n",
       " 253: 'money',\n",
       " 254: 'rude',\n",
       " 255: 'world',\n",
       " 256: 'dish',\n",
       " 257: 'day',\n",
       " 258: 'taste',\n",
       " 259: 'know',\n",
       " 260: 'took',\n",
       " 261: 'NY',\n",
       " 262: 'into',\n",
       " 263: 'lunch',\n",
       " 264: 'When',\n",
       " 265: 'There',\n",
       " 266: 'everything',\n",
       " 267: 'definitely',\n",
       " 268: 'quality',\n",
       " 269: 'For',\n",
       " 270: 'family',\n",
       " 271: 'quite',\n",
       " 272: 'appetizer',\n",
       " 273: 'expensive',\n",
       " 274: 'another',\n",
       " 275: 'least',\n",
       " 276: 'fast',\n",
       " 277: 'birthday',\n",
       " 278: 'side',\n",
       " 279: 'rolls',\n",
       " 280: 'since',\n",
       " 281: 'recommended',\n",
       " 282: 'after',\n",
       " 283: 'things',\n",
       " 284: 'boyfriend',\n",
       " 285: 'dining',\n",
       " 286: 'where',\n",
       " 287: 'next',\n",
       " 288: 'And',\n",
       " 289: \"'ll\",\n",
       " 290: 'neighborhood',\n",
       " 291: 'live',\n",
       " 292: 'new',\n",
       " 293: 'priced',\n",
       " 294: 'enough',\n",
       " 295: 'reasonable',\n",
       " 296: 'Best',\n",
       " 297: 'huge',\n",
       " 298: 'Japanese',\n",
       " 299: 'warm',\n",
       " 300: 'while',\n",
       " 301: 'selection',\n",
       " 302: 'Pizza',\n",
       " 303: \"'d\",\n",
       " 304: 'left',\n",
       " 305: 'date',\n",
       " 306: 'actually',\n",
       " 307: 'few',\n",
       " 308: 'manager',\n",
       " 309: 'Manhattan',\n",
       " 310: '3',\n",
       " 311: 'trip',\n",
       " 312: \"'\",\n",
       " 313: 'give',\n",
       " 314: 'thought',\n",
       " 315: 'ate',\n",
       " 316: 'looking',\n",
       " 317: 'however',\n",
       " 318: 'evening',\n",
       " 319: 'reviews',\n",
       " 320: 'bland',\n",
       " 321: 'cold',\n",
       " 322: 'cheese',\n",
       " 323: 'overpriced',\n",
       " 324: 'big',\n",
       " 325: 'without',\n",
       " 326: 'fantastic',\n",
       " 327: 'keep',\n",
       " 328: 'Try',\n",
       " 329: 'found',\n",
       " 330: 'especially',\n",
       " 331: 'served',\n",
       " 332: 'anything',\n",
       " 333: 'absolutely',\n",
       " 334: 'course',\n",
       " 335: 'late',\n",
       " 336: 'ok',\n",
       " 337: 'sauce',\n",
       " 338: 'nt',\n",
       " 339: 'same',\n",
       " 340: 'ambience',\n",
       " 341: 'Went',\n",
       " 342: 'outside',\n",
       " 343: 'sake',\n",
       " 344: 'spicy',\n",
       " 345: 'pleasant',\n",
       " 346: 'area',\n",
       " 347: 'half',\n",
       " 348: 'La',\n",
       " 349: 'Go',\n",
       " 350: 'still',\n",
       " 351: 'large',\n",
       " 352: 'pay',\n",
       " 353: 'work',\n",
       " 354: 'often',\n",
       " 355: 'town',\n",
       " 356: 'cheap',\n",
       " 357: 'see',\n",
       " 358: 'husband',\n",
       " 359: 'bagel',\n",
       " 360: 'real',\n",
       " 361: 'wanted',\n",
       " 362: 'eating',\n",
       " 363: 'tasted',\n",
       " 364: 'ask',\n",
       " 365: 'her',\n",
       " 366: 'finally',\n",
       " 367: '/',\n",
       " 368: 'What',\n",
       " 369: 'In',\n",
       " 370: 'yet',\n",
       " 371: 'value',\n",
       " 372: 'Also',\n",
       " 373: 'Just',\n",
       " 374: 'However',\n",
       " 375: 'hour',\n",
       " 376: 'Our',\n",
       " 377: 'Italian',\n",
       " 378: 'glass',\n",
       " 379: 'sat',\n",
       " 380: 'appetizers',\n",
       " 381: 'part',\n",
       " 382: 'lobster',\n",
       " 383: 'return',\n",
       " 384: 'bagels',\n",
       " 385: '5',\n",
       " 386: 'horrible',\n",
       " 387: 'room',\n",
       " 388: 'Shabu',\n",
       " 389: 'bill',\n",
       " 390: 'beautiful',\n",
       " 391: 'Good',\n",
       " 392: 'now',\n",
       " 393: 'location',\n",
       " 394: 'portions',\n",
       " 395: 'Brooklyn',\n",
       " 396: 'disappointed',\n",
       " 397: 'three',\n",
       " 398: 'wrong',\n",
       " 399: 'rather',\n",
       " 400: 'need',\n",
       " 401: 'shrimp',\n",
       " 402: 'Do',\n",
       " 403: 'home',\n",
       " 404: 'check',\n",
       " 405: 'deal',\n",
       " 406: 'prepared',\n",
       " 407: 'simply',\n",
       " 408: 'almost',\n",
       " 409: 'sandwich',\n",
       " 410: 'sometimes',\n",
       " 411: 'cozy',\n",
       " 412: 'someone',\n",
       " 413: 'wife',\n",
       " 414: 'As',\n",
       " 415: 'lived',\n",
       " 416: 'Village',\n",
       " 417: 'each',\n",
       " 418: 'flavor',\n",
       " 419: 'prompt',\n",
       " 420: 'may',\n",
       " 421: 'salad',\n",
       " 422: 'fine',\n",
       " 423: 'All',\n",
       " 424: 'person',\n",
       " 425: 'reservation',\n",
       " 426: 'although',\n",
       " 427: 'pasta',\n",
       " 428: 'felt',\n",
       " 429: 'expect',\n",
       " 430: 'City',\n",
       " 431: 'bite',\n",
       " 432: 'either',\n",
       " 433: 'makes',\n",
       " 434: 'salmon',\n",
       " 435: 'his',\n",
       " 436: 'entree',\n",
       " 437: 'drink',\n",
       " 438: 'arrived',\n",
       " 439: 'After',\n",
       " 440: 'pork',\n",
       " 441: 'usually',\n",
       " 442: 'cooked',\n",
       " 443: 'cool',\n",
       " 444: 'something',\n",
       " 445: 'upon',\n",
       " 446: 'tuna',\n",
       " 447: 'believe',\n",
       " 448: 'told',\n",
       " 449: 'might',\n",
       " 450: 'probably',\n",
       " 451: 'house',\n",
       " 452: 'gets',\n",
       " 453: 'week',\n",
       " 454: 'worst',\n",
       " 455: 'crust',\n",
       " 456: 'right',\n",
       " 457: 'loved',\n",
       " 458: 'gem',\n",
       " 459: 'inside',\n",
       " 460: 'oily',\n",
       " 461: 'group',\n",
       " 462: 'pricey',\n",
       " 463: 'dumplings',\n",
       " 464: 'piece',\n",
       " 465: 'chef',\n",
       " 466: 'sitting',\n",
       " 467: 'instead',\n",
       " 468: 'So',\n",
       " 469: 'wish',\n",
       " 470: 'belly',\n",
       " 471: 'several',\n",
       " 472: 'cute',\n",
       " 473: 'happy',\n",
       " 474: 'called',\n",
       " 475: 'dine',\n",
       " 476: 'mind',\n",
       " 477: 'high',\n",
       " 478: 'crowded',\n",
       " 479: 'simple',\n",
       " 480: 'review',\n",
       " 481: 'wo',\n",
       " 482: 'One',\n",
       " 483: 'Sushi',\n",
       " 484: 'fact',\n",
       " 485: 'indian',\n",
       " 486: '10',\n",
       " 487: '15',\n",
       " 488: 'paid',\n",
       " 489: 'Chinese',\n",
       " 490: 'main',\n",
       " 491: 'Restaurant',\n",
       " 492: 'four',\n",
       " 493: 'sweet',\n",
       " 494: 'dry',\n",
       " 495: 'point',\n",
       " 496: 'Everything',\n",
       " 497: 'incredible',\n",
       " 498: 'myself',\n",
       " 499: 'serve',\n",
       " 500: 'trying',\n",
       " 501: 'clean',\n",
       " 502: 'look',\n",
       " 503: 'extensive',\n",
       " 504: 'awesome',\n",
       " 505: 'water',\n",
       " 506: 'everyone',\n",
       " 507: 'looked',\n",
       " 508: 'couple',\n",
       " 509: 'slow',\n",
       " 510: 'casual',\n",
       " 511: 'view',\n",
       " 512: 'above',\n",
       " 513: 'soup',\n",
       " 514: 'done',\n",
       " 515: 'ingredients',\n",
       " 516: 'seafood',\n",
       " 517: 'portion',\n",
       " 518: 'fried',\n",
       " 519: 'Have',\n",
       " 520: 'waited',\n",
       " 521: 'end',\n",
       " 522: 'Very',\n",
       " 523: 'sashimi',\n",
       " 524: 'those',\n",
       " 525: 'name',\n",
       " 526: 'packed',\n",
       " 527: 'bread',\n",
       " 528: 'setting',\n",
       " 529: 'fairly',\n",
       " 530: 'extremely',\n",
       " 531: 'wines',\n",
       " 532: 'these',\n",
       " 533: 'full',\n",
       " 534: 'Cafe',\n",
       " 535: 'problem',\n",
       " 536: 'completely',\n",
       " 537: 'through',\n",
       " 538: 'waiters',\n",
       " 539: 'party',\n",
       " 540: 'write',\n",
       " 541: 'size',\n",
       " 542: 'less',\n",
       " 543: 'kind',\n",
       " 544: 'authentic',\n",
       " 545: 'old',\n",
       " 546: 'seen',\n",
       " 547: 'tables',\n",
       " 548: 'music',\n",
       " 549: '4',\n",
       " 550: 'Excellent',\n",
       " 551: 'impressed',\n",
       " 552: 'Love',\n",
       " 553: 'seated',\n",
       " 554: 'meat',\n",
       " 555: 'received',\n",
       " 556: 'style',\n",
       " 557: 'empty',\n",
       " 558: 'tiny',\n",
       " 559: 'walked',\n",
       " 560: 'share',\n",
       " 561: 'Ca',\n",
       " 562: 'stuff',\n",
       " 563: 'street',\n",
       " 564: 'reason',\n",
       " 565: 'hang',\n",
       " 566: 'decent',\n",
       " 567: 'japanese',\n",
       " 568: 'seemed',\n",
       " 569: 'recently',\n",
       " 570: 'establishment',\n",
       " 571: 'crowd',\n",
       " 572: 'quiet',\n",
       " 573: 'stumbled',\n",
       " 574: 'terrific',\n",
       " 575: 'meals',\n",
       " 576: '8',\n",
       " 577: 'hostess',\n",
       " 578: 'getting',\n",
       " 579: 'yummy',\n",
       " 580: 'toppings',\n",
       " 581: 'expected',\n",
       " 582: 'choices',\n",
       " 583: 'disgusting',\n",
       " 584: 'die',\n",
       " 585: 'matter',\n",
       " 586: 'garden',\n",
       " 587: 'helpful',\n",
       " 588: 'open',\n",
       " 589: 'occasion',\n",
       " 590: 'intimate',\n",
       " 591: 'seems',\n",
       " 592: 'Even',\n",
       " 593: 'certainly',\n",
       " 594: 'stop',\n",
       " 595: 'business',\n",
       " 596: 'poor',\n",
       " 597: 'tell',\n",
       " 598: 'including',\n",
       " 599: 'able',\n",
       " 600: 'To',\n",
       " 601: 'sum',\n",
       " 602: 'NEVER',\n",
       " 603: 'cuisine',\n",
       " 604: 'Saturday',\n",
       " 605: 'Will',\n",
       " 606: 'regular',\n",
       " 607: 'reservations',\n",
       " 608: 'limited',\n",
       " 609: 'Overall',\n",
       " 610: 'beer',\n",
       " 611: 'fair',\n",
       " 612: 'except',\n",
       " 613: 'No',\n",
       " 614: 'cost',\n",
       " 615: 'year',\n",
       " 616: 'quick',\n",
       " 617: 'tea',\n",
       " 618: 'Highly',\n",
       " 619: 'totally',\n",
       " 620: 'twice',\n",
       " 621: 'yes',\n",
       " 622: 'remember',\n",
       " 623: 'pleasantly',\n",
       " 624: 'spectacular',\n",
       " 625: 'outstanding',\n",
       " 626: 'delivery',\n",
       " 627: 'waiting',\n",
       " 628: 'spinach',\n",
       " 629: 'Suan',\n",
       " 630: 'bartender',\n",
       " 631: 'hand',\n",
       " 632: 'apology',\n",
       " 633: 'dog',\n",
       " 634: 'dogs',\n",
       " 635: '–',\n",
       " 636: 'Casa',\n",
       " 637: 'Femme',\n",
       " 638: 'put',\n",
       " 639: 'Korean',\n",
       " 640: 'front',\n",
       " 641: 'previous',\n",
       " 642: 'used',\n",
       " 643: 'complimentary',\n",
       " 644: 've',\n",
       " 645: 'replied',\n",
       " 646: 'okay',\n",
       " 647: 'enjoyed',\n",
       " 648: 'across',\n",
       " 649: 'appetizing',\n",
       " 650: 'Their',\n",
       " 651: 'relaxed',\n",
       " 652: 'busy',\n",
       " 653: 'notch',\n",
       " 654: 'slightly',\n",
       " 655: 'She',\n",
       " 656: 'gone',\n",
       " 657: 'vegetarian',\n",
       " 658: 'hungry',\n",
       " 659: 'conversation',\n",
       " 660: 'yourself',\n",
       " 661: 'filling',\n",
       " 662: 'consider',\n",
       " 663: 'hands',\n",
       " 664: 'soggy',\n",
       " 665: 'Yes',\n",
       " 666: 'unless',\n",
       " 667: 'disappointment',\n",
       " 668: 'offers',\n",
       " 669: 'coming',\n",
       " 670: '.....',\n",
       " 671: 'east',\n",
       " 672: 'thai',\n",
       " 673: 'With',\n",
       " 674: 'fix',\n",
       " 675: 'guests',\n",
       " 676: 'visited',\n",
       " 677: 'Mizu',\n",
       " 678: 'customers',\n",
       " 679: 'sit',\n",
       " 680: 'bathroom',\n",
       " 681: 'outdoor',\n",
       " 682: 'own',\n",
       " 683: 'On',\n",
       " 684: 'afternoon',\n",
       " 685: 'professional',\n",
       " 686: 'mid',\n",
       " 687: 'visiting',\n",
       " 688: 'floor',\n",
       " 689: 'walk',\n",
       " 690: 'ago',\n",
       " 691: 'Sea',\n",
       " 692: 'etc',\n",
       " 693: 'particularly',\n",
       " 694: 'hidden',\n",
       " 695: 'having',\n",
       " 696: 'St.',\n",
       " 697: '1',\n",
       " 698: 'soon',\n",
       " 699: 'liked',\n",
       " 700: 'literally',\n",
       " 701: '....',\n",
       " 702: 'sense',\n",
       " 703: 'beef',\n",
       " 704: '*',\n",
       " 705: 'close',\n",
       " 706: 'plates',\n",
       " 707: 'anyone',\n",
       " 708: 'needs',\n",
       " 709: 'seating',\n",
       " 710: 'together',\n",
       " 711: 'seem',\n",
       " 712: 'lamb',\n",
       " 713: 'secret',\n",
       " 714: 'Always',\n",
       " 715: 'Bukhara',\n",
       " 716: 'hard',\n",
       " 717: 'decided',\n",
       " 718: 'overall',\n",
       " 719: 'disappointing',\n",
       " 720: 'brunch',\n",
       " 721: 'mean',\n",
       " 722: 'surprised',\n",
       " 723: 'customer',\n",
       " 724: 'please',\n",
       " 725: 'plate',\n",
       " 726: 'him',\n",
       " 727: 'While',\n",
       " 728: 'months',\n",
       " 729: 'Nice',\n",
       " 730: 'dim',\n",
       " 731: 'corner',\n",
       " 732: 'offered',\n",
       " 733: 'mediocre',\n",
       " 734: 'guys',\n",
       " 735: 'Taiwanese',\n",
       " 736: 'THE',\n",
       " 737: 'Williamsburg',\n",
       " 738: 'feeling',\n",
       " 739: 'space',\n",
       " 740: 'stomach',\n",
       " 741: 'Everyone',\n",
       " 742: 'Four',\n",
       " 743: 'Seasons',\n",
       " 744: 'n’t',\n",
       " 745: 'Prices',\n",
       " 746: 'longer',\n",
       " 747: 'brought',\n",
       " 748: 'noodles',\n",
       " 749: 'Saul',\n",
       " 750: 'Street',\n",
       " 751: 'interesting',\n",
       " 752: 'oysters',\n",
       " 753: 'waste',\n",
       " 754: 'modern',\n",
       " 755: 'listed',\n",
       " 756: 'tempura',\n",
       " 757: 'mouth',\n",
       " 758: 'along',\n",
       " 759: 'asking',\n",
       " 760: 'saturday',\n",
       " 761: 'lives',\n",
       " 762: 'nearby',\n",
       " 763: 'gave',\n",
       " 764: '6',\n",
       " 765: 'kitchen',\n",
       " 766: 'offer',\n",
       " 767: 'cake',\n",
       " 768: 'Once',\n",
       " 769: 'French',\n",
       " 770: 'desserts',\n",
       " 771: 'delivered',\n",
       " 772: 'use',\n",
       " 773: 'fancy',\n",
       " 774: 'super',\n",
       " 775: 'specials',\n",
       " 776: 'comes',\n",
       " 777: 'incredibly',\n",
       " 778: 'seat',\n",
       " 779: '20',\n",
       " 780: 'Chinatown',\n",
       " 781: 'phenomenal',\n",
       " 782: 'cramped',\n",
       " 783: 'west',\n",
       " 784: 'variety',\n",
       " 785: 'long',\n",
       " 786: 'begin',\n",
       " 787: 'ordering',\n",
       " 788: 'nights',\n",
       " 789: 'vibe',\n",
       " 790: 'reasonably',\n",
       " 791: 'maitre',\n",
       " 792: 'delight',\n",
       " 793: 'Jekyll',\n",
       " 794: 'server',\n",
       " 795: 'potatoes',\n",
       " 796: 'thin',\n",
       " 797: 'quickly',\n",
       " 798: 'By',\n",
       " 799: 'Over',\n",
       " 800: '100',\n",
       " 801: 'Sunday',\n",
       " 802: 'theater',\n",
       " 803: 'during',\n",
       " 804: 'generally',\n",
       " 805: 'Red',\n",
       " 806: 'Lobster',\n",
       " 807: 'Grill',\n",
       " 808: 'upscale',\n",
       " 809: 'walking',\n",
       " 810: 'village',\n",
       " 811: 'anniversary',\n",
       " 812: 'surprise',\n",
       " 813: 'interior',\n",
       " 814: 'cream',\n",
       " 815: 'chips',\n",
       " 816: 'Toons',\n",
       " 817: 'pad',\n",
       " 818: 'salads',\n",
       " 819: 'chance',\n",
       " 820: 'fabulous',\n",
       " 821: 'Although',\n",
       " 822: 'inexpensive',\n",
       " 823: 'extra',\n",
       " 824: 'Big',\n",
       " 825: 'goat',\n",
       " 826: 'chocolate',\n",
       " 827: 'consistent',\n",
       " 828: 'means',\n",
       " 829: 'mushroom',\n",
       " 830: 'guess',\n",
       " 831: 'started',\n",
       " 832: 'relax',\n",
       " 833: 'steak',\n",
       " 834: 'filet',\n",
       " 835: 'looks',\n",
       " 836: 'affordable',\n",
       " 837: 'typical',\n",
       " 838: 'start',\n",
       " 839: 'joint',\n",
       " 840: 'somewhere',\n",
       " 841: 'itself',\n",
       " 842: 'ambiance',\n",
       " 843: 'bottle',\n",
       " 844: 'bottles',\n",
       " 845: 'servers',\n",
       " 846: 'tasting',\n",
       " 847: 'entrees',\n",
       " 848: 'Delivery',\n",
       " 849: '7',\n",
       " 850: 'owner',\n",
       " 851: 'green',\n",
       " 852: 'exceptional',\n",
       " 853: 'low',\n",
       " 854: 'rush',\n",
       " 855: 'Most',\n",
       " 856: 'heard',\n",
       " 857: 'past',\n",
       " 858: 'Park',\n",
       " 859: 'apart',\n",
       " 860: 'THIS',\n",
       " 861: 'PLACE',\n",
       " 862: 'nearly',\n",
       " 863: 'crab',\n",
       " 864: 'Authentic',\n",
       " 865: 'Be',\n",
       " 866: 'maybe',\n",
       " 867: 'set',\n",
       " 868: 'Never',\n",
       " 869: 'Loved',\n",
       " 870: 'girlfriend',\n",
       " 871: 'barely',\n",
       " 872: 'included',\n",
       " 873: 'creative',\n",
       " 874: 'mashed',\n",
       " 875: 'slice',\n",
       " 876: 'tasteless',\n",
       " 877: 'says',\n",
       " 878: 'Too',\n",
       " 879: 'free',\n",
       " 880: 'LOVE',\n",
       " 881: 'summer',\n",
       " 882: 'run',\n",
       " 883: 'scene',\n",
       " 884: 'alone',\n",
       " 885: 'pre',\n",
       " 886: 'BBQ',\n",
       " 887: 'head',\n",
       " 888: 'missing',\n",
       " 889: 'penne',\n",
       " 890: 'guy',\n",
       " 891: 'West',\n",
       " 892: 'spend',\n",
       " 893: 'Bark',\n",
       " 894: 'maggot',\n",
       " 895: 'glasses',\n",
       " 896: 'Flatbush',\n",
       " 897: '500',\n",
       " 898: 'At',\n",
       " 899: 'Dokebi',\n",
       " 900: 'margherita',\n",
       " 901: 'level',\n",
       " 902: 'requests',\n",
       " 903: 'lousy',\n",
       " 904: 'salty',\n",
       " 905: 'tip',\n",
       " 906: 'consistently',\n",
       " 907: 'oyster',\n",
       " 908: 'Fish',\n",
       " 909: 'glad',\n",
       " 910: 'beat',\n",
       " 911: 'perfection',\n",
       " 912: 'fusion',\n",
       " 913: 'Grilled',\n",
       " 914: 'Chicken',\n",
       " 915: 'salt',\n",
       " 916: 'REALLY',\n",
       " 917: 'Haru',\n",
       " 918: 'between',\n",
       " 919: 'Friday',\n",
       " 920: 'THe',\n",
       " 921: 'leaves',\n",
       " 922: 'company',\n",
       " 923: 'york',\n",
       " 924: 'buttery',\n",
       " 925: 'buffet',\n",
       " 926: 'girlfriends',\n",
       " 927: 'idea',\n",
       " 928: 'impressive',\n",
       " 929: 'IT',\n",
       " 930: 'shredded',\n",
       " 931: 'overrated',\n",
       " 932: 'knows',\n",
       " 933: 'life',\n",
       " 934: 'Was',\n",
       " 935: 'Price',\n",
       " 936: 'waitstaff',\n",
       " 937: 'ALL',\n",
       " 938: 'drive',\n",
       " 939: 'East',\n",
       " 940: 'fare',\n",
       " 941: 'weather',\n",
       " 942: 'winter',\n",
       " 943: 'kids',\n",
       " 944: 'weekends',\n",
       " 945: 'hype',\n",
       " 946: 'short',\n",
       " 947: 'read',\n",
       " 948: 'taken',\n",
       " 949: 'care',\n",
       " 950: 'block',\n",
       " 951: 'tired',\n",
       " 952: 'white',\n",
       " 953: 'finish',\n",
       " 954: 'First',\n",
       " 955: 'rushed',\n",
       " 956: 'Steak',\n",
       " 957: 'Patsy',\n",
       " 958: 'reviewer',\n",
       " 959: 'mom',\n",
       " 960: 'anywhere',\n",
       " 961: 'exactly',\n",
       " 962: 'five',\n",
       " 963: 'star',\n",
       " 964: 'missed',\n",
       " 965: 'BUT',\n",
       " 966: 'moved',\n",
       " 967: 'toilet',\n",
       " 968: 'mess',\n",
       " 969: 'stay',\n",
       " 970: 'Hyde',\n",
       " 971: 'whole',\n",
       " 972: 'watching',\n",
       " 973: 'feet',\n",
       " 974: 'line',\n",
       " 975: 'mozzarella',\n",
       " 976: 'frozen',\n",
       " 977: 'pie',\n",
       " 978: 'UWS',\n",
       " 979: 'relaxing',\n",
       " 980: 'truly',\n",
       " 981: '1st',\n",
       " 982: 'Ave',\n",
       " 983: 'original',\n",
       " 984: 'second',\n",
       " 985: 'lovers',\n",
       " 986: 'Uni',\n",
       " 987: 'He',\n",
       " 988: 'Tuna',\n",
       " 989: 'non',\n",
       " 990: 'Wine',\n",
       " 991: 'filled',\n",
       " 992: 'hall',\n",
       " 993: 'noisy',\n",
       " 994: 'Spice',\n",
       " 995: 'fill',\n",
       " 996: 'noticed',\n",
       " 997: 'true',\n",
       " 998: 'private',\n",
       " ...}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
